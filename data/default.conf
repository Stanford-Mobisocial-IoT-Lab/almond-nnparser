#
# Configuration file for NN-SEMPRE
#
# All scripts read a "default.conf" file from
# current directory, and a "model.conf" file from
# model directory
#
# This file is just an example. All values are the default,
# hardcoded in models/config.py
# 

[model]
# Type of model (seq2seq or beamsearch)
#model_type = seq2seq
# Type of encoder (rnn, birnn or bagofwords)
#encoder_type = birnn
# Type of RNN cell (lstm, gru or basic-tanh)
#rnn_cell_type = lstm

# Size of the RNN state (h)
#encoder_hidden_size = 35
#decoder_hidden_size = 70
#apply_attention = true
#rnn_layers = 1

[training]
#n_epochs = 20
#learning_rate = 0.001
#dropout = 0.5
#batch_size = 256

[input]
#input_words = ./input_words.txt
#input_embeddings = ./embeddings-300.txt
#input_embed_size = 300
#train_input_embeddings = false
# Extend the embeddings to account for special tokens (NUMBER_0, etc.) 
#use_typed_embeddings = true

# Max sentence length
#max_length = 60

[output]
# The output language to use (tt or simple)
#grammar = tt
#grammar_input_file = ./thingpedia.txt

# Whether to apply grammar constraints to the inference
# decoder
#use_grammar_constraints = true

# Options for beam decoding; beam width is ignored if
# model_type is not beamsearch
#use_beam_decode = false
#beam_width = 10

# Whether to embed output tokens or use one-hot vectors
# output_embed_size is ignored if train_output_embeddings
# is false
# use_typed_embeddings is ignored if train_output_embeddings
# is true
#train_output_embeddings = true
#output_embed_size = 15
#use_typed_embeddings = true

