#
# Configuration file for almond-nnparser
#
# All scripts read a "default.conf" file from
# current directory, and a "model.conf" file from
# the model directory.
#
# This file is just an example. All values are the default,
# hardcoded in models/config.py
# 

[model]
# Type of model (seq2seq, beamdecode, beamsearch, rpag or transformer)
# NOTE: only seq2seq and beamdecode are officially supported, the others might or might not work
# NOTE: training with beamdecode is not guaranteed to work; you should train with seq2seq and then
# switch to beamdecode later
#model_type = seq2seq
# Type of encoder (birnn, rnn, bagofwords, tree)
# NOTE: tree is not completely supported and might not work; rnn and bagofwords have lower performance 
#encoder_type = birnn
# Type of RNN cell (lstm, gru or basic-tanh)
# NOTE: cells other than lstm have significantly lower performance
#rnn_cell_type = lstm

# Size of the RNN state (h)
#encoder_hidden_size = 125
#decoder_hidden_size = 250
#apply_attention = true
#rnn_layers = 1

[training]
#n_epochs = 25
#learning_rate = 0.005
#learning_rate_decay = 0.95
#dropout = 0.5
#batch_size = 256
#shuffle_data = true
#l2_regularization = 0.0
#l1_regularization = 0.0,
# Whether to apply gradient clipping (0 disables)
#gradient_clip = 1.0
# L2 regularization applied only to embedding matrices
#embedding_l2_regularization=0.0
# Scheduled sampling probability increase per epoch (0 disables scheduled sampling)
#scheduled_sampling=0.0
# Apply a loss term based on how many actions are predicted (0 disables)
#decoder_action_count_loss=0.0
# Apply a loss term based on the Hamming distance of the predicted sequence and the gold sequence
# (this is the default loss for a sequence-to-sequence model) 
#decoder_sequence_loss=1.0
# The optimizer to use. Must be an Optimizer class in the tensorflow.train package, eg RMSProp or Adam
#optimizer = RMSProp
# Whether to use softmax or max-margin loss; not all models support this
#use_margin_loss = true

[input]
#input_words = ./en/input_words.txt
#input_embeddings = ./en/embeddings-300.txt
#input_embed_size = 300
#input_projection = 75
#train_input_embeddings = false
# Extend the embeddings to account for special tokens (NUMBER_0, etc.) 
#use_typed_embeddings = true
# Max sentence or program length
#max_length = 65

[output]
# The output language to use; must be a class in the grammar package
# (other values are also allowed here for compatibility)
#grammar = thingtalk.ThingTalkGrammar
#grammar_input_file = ./en/thingpedia.json

# Options for beam search; beam width is ignored if
# model_type is not beamsearch or beamdecode
# beam_width controls inference time, training_beam_width controls training time
#beam_width = 10
#training_beam_width = 10

# Whether to embed output tokens or use one-hot vectors
# output_embed_size is ignored if train_output_embeddings
# is false
# use_typed_embeddings is ignored if train_output_embeddings
# is true
#train_output_embeddings = true
#output_embed_size = 100
#use_typed_embeddings = true
