[model]
model_type = seq2seq
encoder_type = birnn
encoder_hidden_size = 75
decoder_hidden_size = 150
rnn_cell_type = lstm
rnn_layers = 1
apply_attention = true

[training]
batch_size = 256
n_epochs = 15
learning_rate = 0.005
dropout = 0.5
gradient_clip = 1.0
embedding_l2_regularization = 0.0
l2_regularization = 0.0
l1_regularization = 0.0
optimizer = RMSProp
use_margin_loss = true

[input]
input_words = ./input_words.txt
input_embeddings = ./embeddings-300.txt
input_embed_size = 300
input_projection = 50
max_length = 60
train_input_embeddings = false
use_typed_embeddings = true

[output]
grammar = new-tt
grammar_input_file = thingpedia.json
train_output_embeddings = true
output_embed_size = 50
use_typed_embeddings = true
use_dot_product_output = false
beam_width = 10