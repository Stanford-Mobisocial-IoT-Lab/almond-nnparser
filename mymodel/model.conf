[model]
model_type = seq2seq
encoder_type = birnn
encoder_hidden_size = 35
decoder_hidden_size = 70
rnn_cell_type = lstm
rnn_layers = 1
apply_attention = true
attention_probability_fn = softmax

[training]
batch_size = 256
n_epochs = 25
learning_rate = 0.01
learning_rate_decay = 0.9
dropout = 0.5
gradient_clip = 0.5
l2_regularization = 0.0
l1_regularization = 0.0
embedding_l2_regularization = 0.0
scheduled_sampling = 0.0
decoder_action_count_loss = 0.0
decoder_sequence_loss = 1.0
optimizer = RMSProp
shuffle_data = true
use_margin_loss = true

[input]
input_words = ./input_words.txt
input_embeddings = ./embeddings-300.txt
input_embed_size = 300
input_projection = 50
max_length = 60
train_input_embeddings = false
use_typed_embeddings = true

[output]
grammar = tt
grammar_input_file = ./thingpedia.json
train_output_embeddings = true
output_embed_size = 15
beam_width = 10
training_beam_width = 10

